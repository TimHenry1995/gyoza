{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Flow Models\n",
    "\n",
    "![Image](https://github.com/TimHenry1995/gyoza/blob/develop/docs/images/Modelling%20Cycle.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background\n",
    "Flow models are invertible artificial neural networks that can be used for various tasks including regression, probability densitiy estimation and latent factor disentanglement. Similar to regular neural networks, flow networks can be used for classification, as has been shown for instance for optical character recognition [[4](#References)]. Yet, the real contribuiton lies in the efficient invertibility. As first demonstrated by Rezende and Shakir in the year 2015 [[1](#References)], it is possible to feed a sample from a normal distribution through an inverse flow model to obtain a sample from an arbitrary desired data distribution. It thus allows, for instance, to generate photorealistic pictures of human faces [[2](#References)], animals [[2,3,4](#References)] or furniture [[3](#References)]. Apart from that, flow models compare to principal component analysis, or independent component analysis in that they can be used to disentangle latent factors [[4,5](#References)]. Esser, Rombach and Ommer demonstrated in the year 2020 how complicated manifolds in the latent space of regular artificial neural networks can be decomposed into comprehensive factors. They showed how traversal along those factors corresponds to changes in e.g. a person's hairstyle, smile or gender. Similar work has been done by Liu and colleagues in 2022 on medical images of the heart and brain [[5](#References)]. Flow models are still in their infancy and further develoexperiments are needed to better understand their inner workings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Competences Acquired In This Tutorial \n",
    "In this tutorial, the basic modelling cyle will be demonstrated with the help of the flow model package [gyoza](https://gyoza.readthedocs.io/en/latest/). Upon completion, the reader will be able to \n",
    "1. explain the basic properties of flow models\n",
    "2. build sequential flow models with various layers\n",
    "3. calibrate a model with synthetic data\n",
    "4. save and load it for distribution \n",
    "5. use that model for inference and"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Related Documentation\n",
    "At the heart of the gyoza package lays the *flow_layers* module which defines among others, normalization, reflection and coupling layers. The coupling layers rely on standard neural networks found in the *standard_layers* module as well as permutation layers (*flow_layer* module) and their corresponding masks (*masks* module). It is recommended to open the [documentation](https://gyoza.readthedocs.io/en/latest/) in a web browser and read it side-by-side with this tutorial. The corresponding pages will be highlighted in the following sections. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gyoza Installation\n",
    "The gyoza package is built on top of [tensorflow](https://www.tensorflow.org) and has been developed with a minium number of required packages to simplify installation. It is distributed via the python package index and can thus be installed via pip. In case of installation problems, the reader is invited to contact the developer Tim Dick via [GitHub](https://github.com/TimHenry1995/gyoza/tree/main)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install gyoza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gyoza.modelling import flow_layers as mfl, masks as mm, standard_layers as msl\n",
    "import numpy as np, tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building\n",
    "The model bulding routine involves constructing layers and composing them into more complicated models. The approach is thus similar to that of [tensorflow](https://www.tensorflow.org) and [keras](https://keras.io). For readers unfamiliar with the different types of flow layers, it is recommended to review relevant [literature](#references). In the next code snippets, example data is structured as in image processing and fed through different layers. \n",
    "\n",
    "An important aspect of flow layers is that the dimensionality (total number of dimensions spread across all axes) does not change. This is different from regular artificial neural networks which allow for projections from high-dimensional input spaces to low dimensional outputs spaces, e.g. in image classification. A standard approach when modelling with gyoza is to specify along which axes (and corresponding input shape along those axes) the transformations shall be applied. One could for instance limit transformation to spatial, temporal or channel axes. \n",
    "\n",
    "Another important aspect of flow layers is their ability to compute their own Jacobian determinant and to provide its own inverse. This is useful for density estimation of probability distributions when working with the change of variables formula during sampling as demonstrated by Dinh, Sohl-Dickstein and Bengio [[3](#references)]. It is also useful for latent factor disentanglement as shown by Esser, Rombach and Ommer [[4](#references)]. *Documentation: Search the [modelling page](https://gyoza.readthedocs.io/en/latest/gyoza.modelling.html) for FlowLayer invert() and compute_jacobian_determinant().*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "instance_count = 10; width = 8; height = 8; channel_count = 3\n",
    "shape = [instance_count, width, height, channel_count]\n",
    "dimensionality = np.product(shape[1:])\n",
    "X = np.random.normal(loc=3, scale=5, size=shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activation Normalization**\n",
    "\n",
    "The activation normalization layer uses a location and a scale parameter to perform an affine transformation of the data. These two parameters are initialized on the first batch that is passed through the layer to create zero mean and unit standard deviation. During calibration, these parameters can change to more optimal values. *Documentation: Search the [modelling page](https://gyoza.readthedocs.io/en/latest/gyoza.modelling.html) for ActivationNormalization.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initially, X has mean 2.87 and standard deviation 4.95\n",
      "After the layer, it has mean -0.00 and standard deviation 1.00\n"
     ]
    }
   ],
   "source": [
    "# Activation Normalization\n",
    "activation_normalization = mfl.ActivationNormalization(axes=[1,2], shape=[width, height]) # Axes 1,2 are the width and height axes, of the data\n",
    "Y_hat = activation_normalization(X)\n",
    "\n",
    "# Print\n",
    "print(\"Initially, X has mean {mean:.2f} and standard deviation {std:.2f}\".format(mean=np.mean(np.reshape(X,[instance_count, width*height, channel_count])), std=np.std(np.reshape(X,[instance_count, width*height, channel_count]))))\n",
    "print(\"After the layer, it has mean {mean:.2f} and standard deviation {std:.2f}\".format(mean=np.mean(np.reshape(Y_hat,[instance_count, width*height, channel_count])), std=np.std(np.reshape(Y_hat,[instance_count, width*height, channel_count]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reflection**\n",
    "\n",
    "The reflection layer uses [Householder reflections](https://en.wikipedia.org/wiki/Householder_transformation) to reflect its input about a reflection axis. A single reflection layer can to multiple reflections in succession which allows for rotation. Inputs are flattened along the shape input before reflection and unflattened afterwards. *Documentation: Search the [modelling page](https://gyoza.readthedocs.io/en/latest/gyoza.modelling.html) for Reflection.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input to reflection has shape (instance count, width, height, channel count):  (10, 8, 8, 3)\n",
      "The output of reflection has shape (instance count, width, height, channel count):  (10, 8, 8, 3)\n"
     ]
    }
   ],
   "source": [
    "# Reflection\n",
    "reflection = mfl.Reflection(axes=[3], shape=[channel_count], reflection_count=2)\n",
    "\n",
    "# Print\n",
    "print(\"The input to reflection has shape (instance count, width, height, channel count): \", X.shape)\n",
    "Y_hat = reflection(X)\n",
    "print(\"The output of reflection has shape (instance count, width, height, channel count): \", Y_hat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coupling**\n",
    "\n",
    "The coupling layer is the heart of flow networks in that it allows for non-linear transformations that are still trivially invertible. With the help of a binary mask it takes half of the dimensions of its input. It then uses a regular artificial neural network to compute coupling parameters to transform the other half of input dimensions. That last transformation is usually trivial, e.g. addition or scaling to ensure simple inversion. The first half of input dimensions is then concatenated with that transformed second half and returned as output. It is important to realize, that the layer is trivially invertible, because the first half of the output can be fed through the regular neural network to obtain the very same coupling parameters that were computed in the forward operation. \n",
    "\n",
    "Some useful tips for this layer include:\n",
    "- Ensure that the mask and the coupling layer receive the same axes and shape during construction.\n",
    "- Choose a model to compute the coupling parameters whose output shape is the same as the input shape. \n",
    "- The model for computing coupling parameters should integrate dimensions along the same axes as the mask and coupling layer. That is, if mask and and coupling layer act on a single axis, e.g. the channel axis, then a  dense layer is suited for computing coupling paramters. If a Conv2D layer is used to compute coupling parameters, then mask and coupling layer should act on the same three (2 spatial, one channel) axes as that Conv2D layer.\n",
    "\n",
    "*Documentation: Search the [modelling page](https://gyoza.readthedocs.io/en/latest/gyoza.modelling.html) for Coupling, AdditiveCoupling and AffineCoupling.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the input shape to coupling (instance count, width, height, channel count):  (10, 8, 8, 3)\n",
      "This is the input shape to coupling (instance count, width, height, channel count):  (10, 8, 8, 3)\n"
     ]
    }
   ],
   "source": [
    "# Set up a mask to select half of the dimensions\n",
    "mask = mm.CheckerBoard(axes=[1,2,3], shape=[width,height, channel_count])\n",
    "\n",
    "# Set up a regular artificial neural network to compute coupling parameters\n",
    "compute_coupling_parameters = tf.keras.layers.Conv2D(filters=channel_count, kernel_size=(3,3), padding='same')\n",
    "\n",
    "# Coupling\n",
    "coupling = mfl.AdditiveCoupling(axes=[1,2,3], shape=[width, height, channel_count], compute_coupling_parameters=compute_coupling_parameters, mask=mask)\n",
    "\n",
    "# Print\n",
    "print(\"The input to coupling has shape (instance count, width, height, channel count): \", X.shape)\n",
    "Y_hat = coupling(X)\n",
    "print(\"The output of coupling has shape  (instance count, width, height, channel count): \", Y_hat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Semi-Coupling:** Coupling layers can be used in succession, if they are interleaved with permutation layers. Since a coupling layer couples one half of dimensions with the other half, it performs semi-coupling. To ensure that all dimensions are transformed, it makes sense to permute them between coupling layers. A simple permutation is performed by the Shuffle layer which flattens its input along the provided axes, shuffles it (deterministically) and restores the original shape. *Documentation: Search the [modelling page](https://gyoza.readthedocs.io/en/latest/gyoza.modelling.html) for Permutation, Shuffle.*\n",
    "\n",
    "**Complete Coupling:** In order to achieve coupling across all dimensions (instead of half of them) it makes sense to combine two coupling layers. The first coupling layer is set up as above, followed by a permutation layer that swops dimensions. Thereafter a second coupling layer is used and finally a permutation layer restores the original ordering of dimensions. Some useful tips for this form of coupling include:\n",
    "- The permutation layer should be matched with the mask of the coupling layer. That is, a heaviside mask (which selects the second half of dimensions after flattening) should be paired with a heaside permutation layer. Similarly, a checkerboard mask (which selects every other dimension after flattening) shall be paired with a checkerboard permutation layer. \n",
    "- Permutation layers shall match the mask and coupling layers according to the axes and corresponding shape along those axes when processing the input.\n",
    "- The first and second coupling layers should combine the two halves of dimensions in a similar way, e.g. both being additive or both being affine coupling layers.\n",
    "- The first and second permutation layers shall be of the same type, e.g. both be heaviside or both be checkerboard.\n",
    "\n",
    "*Documentation: Search the [modelling page](https://gyoza.readthedocs.io/en/latest/gyoza.modelling.html) for Permutation, Heaviside and Checkerboard.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input to permutation has shape (instance count, width, height, channel count):  (10, 8, 8, 3)\n",
      "The output of permutation has shape  (instance count, width, height, channel count):  (10, 8, 8, 3)\n"
     ]
    }
   ],
   "source": [
    "# Permutation\n",
    "shuffle_permute = mfl.Shuffle(axes=[1,2,3], shape=[width,height,channel_count])\n",
    "heaviside_permute = mfl.Heaviside(axes=[1,2,3], shape=[width,height,channel_count])\n",
    "checkerboard_permute = mfl.CheckerBoard(axes=[1,2,3], shape=[width,height,channel_count])\n",
    "\n",
    "Y_hat = shuffle_permute(X)\n",
    "Y_hat = heaviside_permute(X)\n",
    "Y_hat = checkerboard_permute(X)\n",
    "\n",
    "# Print\n",
    "print(\"The input to permutation has shape (instance count, width, height, channel count): \", X.shape)\n",
    "Y_hat = shuffle_permute(X)\n",
    "print(\"The output of permutation has shape  (instance count, width, height, channel count): \", Y_hat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second coupling layer\n",
    "compute_coupling_parameters_2 = tf.keras.layers.Conv2D(filters=channel_count, kernel_size=(3,3), padding='same')\n",
    "coupling_2 = mfl.AdditiveCoupling(axes=[1,2,3], shape=[width, height, channel_count], compute_coupling_parameters=compute_coupling_parameters_2, mask=mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Composition**\n",
    "\n",
    "A complete model can be constructucted by arranging the layers in a sequence. The flow_layers module provides a sequential layer which requires a sequence of flow layers as input.\n",
    "\n",
    "**IMPORTANT:** It is recommended to neither use the Sequential model from keras nor to build custom models with multiple pathways in tensorflow. This is because every gyoza flow layer is able to provide a Jacobian determinant for its forward operation and to invert itself. This functionality would be lost within a keras Sequential model. Within a multi-pathway tensorflow model it is not obvious how to maintain these properties for the overall model and should thus be done by advanced users who have the relevant mathematical background at their disposal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input to model has shape (instance count, width, height, channel count):  (10, 8, 8, 3)\n",
      "The output of model has shape (instance count, width, height, channel count):  (10, 8, 8, 3)\n"
     ]
    }
   ],
   "source": [
    "# Compose model\n",
    "model = mfl.SequentialFlowNetwork(sequence=[activation_normalization, reflection, coupling, shuffle_permute, coupling_2])\n",
    "\n",
    "# Print\n",
    "print(\"The input to model has shape (instance count, width, height, channel count): \", X.shape)\n",
    "Y_hat = model(X)\n",
    "print(\"The output of model has shape (instance count, width, height, channel count): \", Y_hat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain that model.fit or fit generator does not work or does it ?. Need custom fitting cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain how best to save and load. Do not save the entire model. Only its weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain how to infer. It is just a single instance, no pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "[1] [\"Variational Inference with Normalizing Flows\" by Danilo Jimenez Rezende and Shakir Mohamed](https://arxiv.org/abs/1505.05770)\n",
    "\n",
    "[2] [\"Glow: Generative Flow with Invertible 1×1 Convolutions\" by Diederik P. Kingma, Prafulla Dhariwal](https://arxiv.org/abs/1807.03039)\n",
    "\n",
    "[3] [\"DENSITY ESTIMATION USING REAL NVP\" by Laurent Dinh, Jascha Sohl-Dickstein, Samy Bengio](https://arxiv.org/abs/1605.08803)\n",
    "\n",
    "[4] [\"A Disentangling Invertible Interpretation Network for Explaining Latent Representations\" by Patrick Esser, Robin Rombach, Björn Ommer](https://arxiv.org/abs/2004.13166)\n",
    "\n",
    "[5] [\"Learning disentangled representations in the imaging domain\" by Xiao Liu, Pedro Sanchez, Spyridon Thermos, Alison Q. O'Neil, Sotirios A. Tsaftaris](https://arxiv.org/abs/2108.12043)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gyoza_environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
